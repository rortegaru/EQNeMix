{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read, Stream\n",
    "import seisbench\n",
    "import seisbench.models as sbm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se Carga el modelo pre-entrenado de eqtrnasformes. Hay otras opciones de versiones de este modelo entrenado con otros datos, pero algunas ya no estan disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 18:04:37,611 | seisbench | WARNING | Setting remote root to: https://seisbench.gfz-potsdam.de/mirror/\n",
      "Please note that this can affect your download speed.\n"
     ]
    }
   ],
   "source": [
    "seisbench.use_backup_repository()\n",
    "eq_model = sbm.EQTransformer.from_pretrained(\"stead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seleciona el directorio donde se encuentran los archivos .mseed, estos tienen la caracteristica de que cada componente tiene su propio archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/roberto/SDS2/RAE96/\"\n",
    "\n",
    "df_acumulado = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciona el tango de fechas que se quieren procesar recordando que la nomenclatura se encuentra en dias del año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(122,289):\n",
    "    print(i)\n",
    "    pattern_E = directory + \"RAE96_EHE_2024_\"+str(i)+\".mseed\"\n",
    "    pattern_N = directory + \"RAE96_EHN_2024_\"+str(i)+\".mseed\"\n",
    "    pattern_Z = directory + \"RAE96_EHZ_2024_\"+str(i)+\".mseed\"\n",
    "\n",
    "    # Para automarizar el Proceso, hay veces que los dias no se encuentran en la carpeta, y de esta manera se evita que los archivos faltantes paren la secuencia {\n",
    "    try:\n",
    "        stream_E = read(pattern_E)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {pattern_E} not found\")\n",
    "        stream_E = None\n",
    "\n",
    "    try:\n",
    "        stream_N = read(pattern_N)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {pattern_N} not found\")\n",
    "        stream_N = None\n",
    "\n",
    "    try:\n",
    "        stream_Z = read(pattern_Z)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {pattern_Z} not found\")\n",
    "        stream_Z = None\n",
    "\n",
    "    ##### }\n",
    "    # Ya que necestiamos todas las trazas para poder hacer este análisis se ponen estas condiciones para construir un objeto stream.{\n",
    "    #(cabe aclarar que en caso de que solo se cuente con una traza Z, esta se puede duplicar para que el stream siga conteniendo 3 trazas)\n",
    "\n",
    "    if stream_E is not None:\n",
    "        if stream_N is not None:\n",
    "            if stream_Z is not None:\n",
    "                stream = stream_E + stream_N + stream_Z\n",
    "    ##}\n",
    "    # Muchas veces la traza tiene el problema de estar segmentada, y asumiendo que solo se procesara cuando se tengan las 3 componentes,\n",
    "    # por la manera en que se organizan estas, se hace esto para seleccionar los segmentos de cada traza correspondiente y se hace otro stream {\n",
    "    every = int(len(stream)/3)\n",
    "    for i in range(0,every):\n",
    "        selected_stream = Stream()\n",
    "        # Add traces by specifying their index or other identifying feature\n",
    "        # For example, if you know the indices:\n",
    "        selected_stream += stream[i]  #  trace of component E\n",
    "        selected_stream += stream[i+every]  #  trace of component N\n",
    "        selected_stream += stream[i+2*every]  #  trace of component Z\n",
    "    ##}\n",
    "\n",
    "    # Ahora que se creo el stream con el que se va a trabajar es NECESARIO que todos estos conincidan en datos, \n",
    "    # por lo que se cortan los streams para que todos tengan la misma longitud.\n",
    "    # De manera que El Maximo tiempo de la traza mas atrasada sea el inicio del stream\n",
    "    # Y el minimo tiempo de la traza mas precoz sea el final.\n",
    "        # Obtener el tiempo de inicio y fin común más corto entre las trazas {\n",
    "        tiempo_inicio_comun = max(traza.stats.starttime for traza in selected_stream)\n",
    "        tiempo_fin_comun = min(traza.stats.endtime for traza in selected_stream)\n",
    "        if tiempo_inicio_comun <= tiempo_fin_comun:\n",
    "            # Recortar el stream al tiempo común\n",
    "            stream_trimmed = selected_stream.slice(starttime=tiempo_inicio_comun, endtime=tiempo_fin_comun)\n",
    "    ##}\n",
    "    # Se hace un Detrend al stream{}\n",
    "            stream_trimmed.detrend(type=\"linear\")\n",
    "            #stream_trimmed.normalize()\n",
    "    \n",
    "    # Es en esta linea que se utiliza el modelo pre-entrenado de EQtransformers.{}\n",
    "            outputs = eq_model.classify(stream_trimmed)\n",
    "            print(outputs)\n",
    "    # Lo que sigue es hace un objeto de dataframe para acomodar los resultados y poder ir acumulando los resultados en otro dataframe.\n",
    "            ############Convirtiendo a dataframes\n",
    "            df = pd.DataFrame(outputs.picks)\n",
    "    # Se verifica que EQtransformers en verdad dio un resultado \n",
    "            if not df.empty:\n",
    "                df = df[0].apply(lambda x: x.__dict__)\n",
    "                df = pd.DataFrame(df.tolist())\n",
    "                df.rename(columns={\"start_time\":'start_time_picks'},inplace=True)\n",
    "                df.rename(columns={\"end_time\":'end_time_picks'},inplace=True)\n",
    "                df_P = df[df['phase']=='P']\n",
    "                df_S = df[df['phase']=='S']\n",
    "    # Lo anterior fue la parte de los picados, pero tambien nos importa el rango de detenccion, que es lo que sigue a continuacion{}\n",
    "                df2 = pd.DataFrame(outputs.detections)\n",
    "                df2 = df2[0].apply(lambda x: x.__dict__)\n",
    "                df2 = pd.DataFrame(df2.tolist())\n",
    "                df2.rename(columns={\"start_time\":'start_time_detections'},inplace=True)\n",
    "                df2.rename(columns={\"end_time\":'end_time_detections'},inplace=True)\n",
    "                df2.rename(columns={\"peak_value\":'peak_value_detections'},inplace=True)\n",
    "    \n",
    "    # Se hace un merge complicado de los dataframes para que coincidan los picados de P y S con un rango de deteccion correspondiente\n",
    "    # Esto se hizo de manera artesanal viendo ejemplos de las detencciones.\n",
    "                ############## Merge de los dataframes\n",
    "                final_merged_df = df2.merge(\n",
    "                    df_P,  # Phase P\n",
    "                    on=\"trace_id\",\n",
    "                    how=\"inner\"\n",
    "                ).merge(\n",
    "                    df_S,  # Phase S\n",
    "                    on=\"trace_id\",\n",
    "                    how=\"inner\",\n",
    "                    suffixes=(\"_P\", \"_S\")\n",
    "                ).query(\"start_time_detections <= end_time_picks_P <= end_time_detections and \"\n",
    "                        \"start_time_detections <= end_time_picks_S <= end_time_detections\")\n",
    "                final_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                final_merged_df['archivo'] = \"RAE96_EHE_2024_\"+str(i)+\".mseed\"\n",
    "                #final_merged_df\n",
    "                ##############################\n",
    "                df_acumulado = pd.concat([df_acumulado, final_merged_df], ignore_index=True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # Si no hay solapamiento, puedes manejar el caso aquí (por ejemplo, imprimir un mensaje o continuar)\n",
    "            print(f\"Rango de tiempo inválido: {tiempo_inicio_comun}, {tiempo_fin_comun} \")\n",
    "\n",
    "df_acumulado.to_csv(\"Picados_loscabos_eqtransformes.csv\",index=False)      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seisbench_joel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
